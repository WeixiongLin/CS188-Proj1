{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import gym\n",
    "import torch.utils.data as data\n",
    "from dynamics import *\n",
    "from controller import *\n",
    "from utils import *\n",
    "from quanser_robots.common import GentlyTerminating\n",
    "import time\n",
    "\n",
    "# datasets:  numpy array, size:[sample number, input dimension]\n",
    "# labels:  numpy array, size:[sample number, output dimension]\n",
    "\n",
    "env_id =\"Qube-v0\" # \"CartPole-v0\"\n",
    "env = GentlyTerminating(gym.make(env_id))\n",
    "config_path = \"config.yml\"\n",
    "config = load_config(config_path)\n",
    "#print_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:209: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if datasets == None:\n",
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:213: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if labels == None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect random dataset shape:  (7630, 7)\n"
     ]
    }
   ],
   "source": [
    "model = DynamicModel(config)\n",
    "\n",
    "data_fac = DatasetFactory(env,config)\n",
    "data_fac.collect_random_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training step per epoch [96]\n",
      "Epoch [50/500], Training Loss: 0.07970656, Test Loss: 0.08558914\n",
      "Epoch [100/500], Training Loss: 0.07005977, Test Loss: 0.08037411\n",
      "Epoch [150/500], Training Loss: 0.06543648, Test Loss: 0.07438301\n",
      "Epoch [200/500], Training Loss: 0.06311109, Test Loss: 0.07371941\n",
      "Epoch [250/500], Training Loss: 0.06101681, Test Loss: 0.07367975\n",
      "Epoch [300/500], Training Loss: 0.06066504, Test Loss: 0.07022873\n",
      "Epoch [350/500], Training Loss: 0.05915687, Test Loss: 0.07054408\n",
      "Epoch [400/500], Training Loss: 0.05823344, Test Loss: 0.06820911\n",
      "Epoch [450/500], Training Loss: 0.05771950, Test Loss: 0.06866388\n",
      "Epoch [500/500], Training Loss: 0.05728819, Test Loss: 0.06751502\n"
     ]
    }
   ],
   "source": [
    "loss = model.train(data_fac.random_trainset,data_fac.random_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config[\"mpc_config\"][\"horizon\"] = 12\n",
    "config[\"mpc_config\"][\"numb_bees\"] = 8\n",
    "config[\"mpc_config\"][\"max_itrs\"] = 20\n",
    "config[\"mpc_config\"][\"gamma\"] = 0.98\n",
    "mpc = MPC(env,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************\n",
      "The reinforce process [0], collecting data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:55: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [0/6], Reward: 1.07203271\n"
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "for itr in range(config[\"dataset_config\"][\"n_mpc_itrs\"]):\n",
    "    t = time.time()\n",
    "    print(\"**********************************************\")\n",
    "    print(\"The reinforce process [%s], collecting data ...\" % itr)\n",
    "    rewards = data_fac.collect_mpc_dataset(mpc,model)\n",
    "    trainset, testset = data_fac.make_dataset()\n",
    "    rewards_list += rewards\n",
    "    \n",
    "    plt.close(\"all\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title('Reward Trend with %s iteration' % itr)\n",
    "    plt.plot(rewards_list)\n",
    "    plt.savefig(\"storage/reward-\" + str(model.exp_number) + \".png\")\n",
    "    print(\"Consume %s s in this iteration\" % (time.time()-t))\n",
    "    loss = model.train(trainset,testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
