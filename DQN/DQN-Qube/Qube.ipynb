{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "*** model configuration ***\n",
      "load_model: true\n",
      "model_path: storage/exp_6.ckpt\n",
      "n_actions: 9\n",
      "n_hidden: 1\n",
      "size_hidden: 256\n",
      "use_cuda: true\n",
      "\n",
      "*** train configuration ***\n",
      "batch_size: 64\n",
      "epsilon_decay: 1000\n",
      "epsilon_final: 0.05\n",
      "epsilon_start: 0.9\n",
      "exp_number: 66\n",
      "fix_epsilon: 0.1\n",
      "gamma: 0.99\n",
      "learning_rate: 0.0001\n",
      "max_episode_step: 500\n",
      "memory_size: 100000\n",
      "n_episodes: 25000\n",
      "n_update_target: 6\n",
      "random_seed: 1234\n",
      "render: false\n",
      "save_best: true\n",
      "save_model_path: storage/exp_6_rr.ckpt\n",
      "save_thres: 510\n",
      "use_fix_epsilon: true\n",
      "\n",
      "************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'DQN.MLP' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CUDA\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from DQN import *\n",
    "import gym\n",
    "from quanser_robots.common import GentlyTerminating\n",
    "\n",
    "\n",
    "'''Load the configuration setttings'''\n",
    "config_path = \"config.yml\"\n",
    "print_config(config_path)\n",
    "config = load_config(config_path)\n",
    "training_config = config[\"training_config\"]\n",
    "seed = training_config[\"random_seed\"]\n",
    "n_episodes = training_config[\"n_episodes\"]\n",
    "max_episode_step = training_config[\"max_episode_step\"]\n",
    "n_update_target = training_config[\"n_update_target\"]\n",
    "exp_number = training_config[\"exp_number\"]\n",
    "save_model_path = training_config[\"save_model_path\"]\n",
    "render_flag = training_config[\"render\"]\n",
    "save_best = training_config[\"save_best\"]\n",
    "save_thres = training_config[\"save_thres\"]\n",
    "\n",
    "'''Use fixed epsilon or use a exponential function decay?'''\n",
    "if training_config[\"use_fix_epsilon\"]:\n",
    "    epsilon_by_frame = lambda frame_idx: training_config[\"fix_epsilon\"]\n",
    "else:\n",
    "    epsilon_start = training_config[\"epsilon_start\"]\n",
    "    epsilon_final = training_config[\"epsilon_final\"]\n",
    "    epsilon_decay = training_config[\"epsilon_decay\"]\n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "'''Environment initialization'''\n",
    "env_id = \"Qube-v0\"\n",
    "env = GentlyTerminating(gym.make(env_id))\n",
    "\n",
    "\n",
    "\n",
    "'''Initialize the DQN algorithm object'''\n",
    "policy = Policy(env,config)\n",
    "\n",
    "\n",
    "                                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_reconstruct: First argument must be a sub-type of ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b7f8166d986b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Restore from a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'storage/data_rrr.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: _reconstruct: First argument must be a sub-type of ndarray"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Restore from a file\n",
    "f = open('storage/data_rrr.pkl', 'rb')\n",
    "data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_buffer = policy.replay_buffer\n",
    "\n",
    "for i in range(len(data_rr)):\n",
    "    replay_buffer.push(data_rr[i][0],data_rr[i][1],data_rr[i][2],data_rr[i][3],data_rr[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "avg_rewards = []\n",
    "epsilons = []\n",
    "\n",
    "'''Training the q-network with n episodes'''\n",
    "for i_episode in range(n_episodes):\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "    state[4:6]/=20\n",
    "    epsilon = epsilon_by_frame(i_episode)\n",
    "    epsilons.append(epsilon)\n",
    "    for step in range(max_episode_step):\n",
    "        if render_flag:\n",
    "            env.render()\n",
    "        '''Choose action'''\n",
    "        action = policy.act(state, epsilon)\n",
    "        f_action = 5*(action-(policy.n_actions-1)/2)/((policy.n_actions-1)/2)\n",
    "        next_state, reward, done, _ = env.step(f_action)\n",
    "        reward = 100*(reward)\n",
    "        next_state[4:6]/=20\n",
    "        policy.replay_buffer.push(state, action[0], reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        if len(policy.replay_buffer) > policy.batch_size:\n",
    "            loss = policy.train()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    all_rewards.append(episode_reward)\n",
    "    avg_rewards.append(np.mean(all_rewards[-10:]))\n",
    "\n",
    "    if i_episode % 50 == 0:\n",
    "        '''Save the results figure every 50 episodes'''\n",
    "        save_fig(i_episode, all_rewards,avg_rewards, losses,epsilons, exp_number)\n",
    "\n",
    "    if i_episode % n_update_target == 0:\n",
    "        '''Update the target network'''\n",
    "        policy.update_target()\n",
    "\n",
    "    policy.save_model(save_model_path)\n",
    "    if save_best and i_episode>100:\n",
    "        ratio = 1.1\n",
    "        if episode_reward > ratio*np.mean(all_rewards[-10:]):\n",
    "            print(\"Save model with episode reward %s \" % (episode_reward))\n",
    "            print(\"Model path: %s \" % (save_model_path))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
