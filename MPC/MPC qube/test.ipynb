{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import gym\n",
    "import torch.utils.data as data\n",
    "from dynamics import *\n",
    "from controller import *\n",
    "from utils import *\n",
    "from quanser_robots.common import GentlyTerminating\n",
    "import time\n",
    "\n",
    "# datasets:  numpy array, size:[sample number, input dimension]\n",
    "# labels:  numpy array, size:[sample number, output dimension]\n",
    "\n",
    "env_id =\"Qube-v0\" # \"CartPole-v0\"\n",
    "env = GentlyTerminating(gym.make(env_id))\n",
    "config_path = \"config.yml\"\n",
    "config = load_config(config_path)\n",
    "#print_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:209: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if datasets == None:\n",
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:213: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if labels == None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect random dataset shape:  (7630, 7)\n"
     ]
    }
   ],
   "source": [
    "model = DynamicModel(config)\n",
    "\n",
    "data_fac = DatasetFactory(env,config)\n",
    "data_fac.collect_random_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training step per epoch [96]\n",
      "Epoch [50/500], Training Loss: 0.07970656, Test Loss: 0.08558914\n",
      "Epoch [100/500], Training Loss: 0.07005977, Test Loss: 0.08037411\n",
      "Epoch [150/500], Training Loss: 0.06543648, Test Loss: 0.07438301\n",
      "Epoch [200/500], Training Loss: 0.06311109, Test Loss: 0.07371941\n",
      "Epoch [250/500], Training Loss: 0.06101681, Test Loss: 0.07367975\n",
      "Epoch [300/500], Training Loss: 0.06066504, Test Loss: 0.07022873\n",
      "Epoch [350/500], Training Loss: 0.05915687, Test Loss: 0.07054408\n",
      "Epoch [400/500], Training Loss: 0.05823344, Test Loss: 0.06820911\n",
      "Epoch [450/500], Training Loss: 0.05771950, Test Loss: 0.06866388\n",
      "Epoch [500/500], Training Loss: 0.05728819, Test Loss: 0.06751502\n"
     ]
    }
   ],
   "source": [
    "loss = model.train(data_fac.random_trainset,data_fac.random_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config[\"mpc_config\"][\"horizon\"] = 12\n",
    "config[\"mpc_config\"][\"numb_bees\"] = 8\n",
    "config[\"mpc_config\"][\"max_itrs\"] = 20\n",
    "config[\"mpc_config\"][\"gamma\"] = 0.98\n",
    "mpc = MPC(env,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************\n",
      "The reinforce process [0], collecting data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:55: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [0/6], Reward: 1.07203271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:251: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if datasets == None:\n",
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:255: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if labels == None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [1/6], Reward: 0.25947355\n",
      "Episode [2/6], Reward: 0.22465315\n",
      "Episode [3/6], Reward: 0.52257896\n",
      "Episode [4/6], Reward: 0.00034281\n",
      "Episode [5/6], Reward: 0.43632075\n",
      "Totally collect 721 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5279 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1351.4746375083923 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.04375696, Test Loss: 0.04690460\n",
      "Epoch [100/500], Training Loss: 0.04186545, Test Loss: 0.04516670\n",
      "Epoch [150/500], Training Loss: 0.04085705, Test Loss: 0.04480868\n",
      "Epoch [200/500], Training Loss: 0.04021174, Test Loss: 0.04422094\n",
      "Epoch [250/500], Training Loss: 0.04018944, Test Loss: 0.04400228\n",
      "Epoch [300/500], Training Loss: 0.03963970, Test Loss: 0.04511513\n",
      "Epoch [350/500], Training Loss: 0.03900105, Test Loss: 0.04449584\n",
      "Epoch [400/500], Training Loss: 0.03942160, Test Loss: 0.04542288\n",
      "Epoch [450/500], Training Loss: 0.03879860, Test Loss: 0.04399608\n",
      "Epoch [500/500], Training Loss: 0.03812746, Test Loss: 0.04389737\n",
      "**********************************************\n",
      "The reinforce process [1], collecting data ...\n",
      "Episode [0/6], Reward: 0.00017768\n",
      "Episode [1/6], Reward: 0.00019406\n",
      "Episode [2/6], Reward: 0.00025889\n",
      "Episode [3/6], Reward: 0.02440372\n",
      "Episode [4/6], Reward: 0.00028506\n",
      "Episode [5/6], Reward: 0.00027441\n",
      "Totally collect 105 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5895 training data from all previous dataset, total training sample: 6000\n",
      "Consume 196.31848788261414 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.04635050, Test Loss: 0.04831853\n",
      "Epoch [100/500], Training Loss: 0.04638940, Test Loss: 0.04861357\n",
      "Epoch [150/500], Training Loss: 0.04594463, Test Loss: 0.04940759\n",
      "Epoch [200/500], Training Loss: 0.04566165, Test Loss: 0.04800418\n",
      "Epoch [250/500], Training Loss: 0.04540235, Test Loss: 0.04845614\n",
      "Epoch [300/500], Training Loss: 0.04454664, Test Loss: 0.04745193\n",
      "Epoch [350/500], Training Loss: 0.04482200, Test Loss: 0.04712954\n",
      "Epoch [400/500], Training Loss: 0.04505566, Test Loss: 0.04657192\n",
      "Epoch [450/500], Training Loss: 0.04417427, Test Loss: 0.04632503\n",
      "Epoch [500/500], Training Loss: 0.04407539, Test Loss: 0.04719584\n",
      "**********************************************\n",
      "The reinforce process [2], collecting data ...\n",
      "Episode [0/6], Reward: 0.00007330\n",
      "Episode [1/6], Reward: 0.00017936\n",
      "Episode [2/6], Reward: 0.00018006\n",
      "Episode [3/6], Reward: 0.00018807\n"
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "for itr in range(config[\"dataset_config\"][\"n_mpc_itrs\"]):\n",
    "    t = time.time()\n",
    "    print(\"**********************************************\")\n",
    "    print(\"The reinforce process [%s], collecting data ...\" % itr)\n",
    "    rewards = data_fac.collect_mpc_dataset(mpc,model)\n",
    "    trainset, testset = data_fac.make_dataset()\n",
    "    rewards_list += rewards\n",
    "    \n",
    "    plt.close(\"all\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title('Reward Trend with %s iteration' % itr)\n",
    "    plt.plot(rewards_list)\n",
    "    plt.savefig(\"storage/reward-\" + str(model.exp_number) + \".png\")\n",
    "    print(\"Consume %s s in this iteration\" % (time.time()-t))\n",
    "    loss = model.train(trainset,testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
