{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import gym\n",
    "import torch.utils.data as data\n",
    "from dynamics import *\n",
    "from controller import *\n",
    "from utils import *\n",
    "from quanser_robots.common import GentlyTerminating\n",
    "import time\n",
    "\n",
    "# datasets:  numpy array, size:[sample number, input dimension]\n",
    "# labels:  numpy array, size:[sample number, output dimension]\n",
    "\n",
    "env_id =\"Qube-v0\" # \"CartPole-v0\"\n",
    "env = GentlyTerminating(gym.make(env_id))\n",
    "config_path = \"config.yml\"\n",
    "config = load_config(config_path)\n",
    "#print_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:209: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if datasets == None:\n",
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:213: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if labels == None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect random dataset shape:  (7630, 7)\n"
     ]
    }
   ],
   "source": [
    "model = DynamicModel(config)\n",
    "\n",
    "data_fac = DatasetFactory(env,config)\n",
    "data_fac.collect_random_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training step per epoch [96]\n",
      "Epoch [50/500], Training Loss: 0.07970656, Test Loss: 0.08558914\n",
      "Epoch [100/500], Training Loss: 0.07005977, Test Loss: 0.08037411\n",
      "Epoch [150/500], Training Loss: 0.06543648, Test Loss: 0.07438301\n",
      "Epoch [200/500], Training Loss: 0.06311109, Test Loss: 0.07371941\n",
      "Epoch [250/500], Training Loss: 0.06101681, Test Loss: 0.07367975\n",
      "Epoch [300/500], Training Loss: 0.06066504, Test Loss: 0.07022873\n",
      "Epoch [350/500], Training Loss: 0.05915687, Test Loss: 0.07054408\n",
      "Epoch [400/500], Training Loss: 0.05823344, Test Loss: 0.06820911\n",
      "Epoch [450/500], Training Loss: 0.05771950, Test Loss: 0.06866388\n",
      "Epoch [500/500], Training Loss: 0.05728819, Test Loss: 0.06751502\n"
     ]
    }
   ],
   "source": [
    "loss = model.train(data_fac.random_trainset,data_fac.random_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config[\"mpc_config\"][\"horizon\"] = 12\n",
    "config[\"mpc_config\"][\"numb_bees\"] = 8\n",
    "config[\"mpc_config\"][\"max_itrs\"] = 20\n",
    "config[\"mpc_config\"][\"gamma\"] = 0.98\n",
    "mpc = MPC(env,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************\n",
      "The reinforce process [0], collecting data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:55: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [0/6], Reward: 1.07203271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:251: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if datasets == None:\n",
      "/home/xinye/projects/rl/RL-project/MPC/MPC qube/dynamics.py:255: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if labels == None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [1/6], Reward: 0.25947355\n",
      "Episode [2/6], Reward: 0.22465315\n",
      "Episode [3/6], Reward: 0.52257896\n",
      "Episode [4/6], Reward: 0.00034281\n",
      "Episode [5/6], Reward: 0.43632075\n",
      "Totally collect 721 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5279 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1351.4746375083923 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.04375696, Test Loss: 0.04690460\n",
      "Epoch [100/500], Training Loss: 0.04186545, Test Loss: 0.04516670\n",
      "Epoch [150/500], Training Loss: 0.04085705, Test Loss: 0.04480868\n",
      "Epoch [200/500], Training Loss: 0.04021174, Test Loss: 0.04422094\n",
      "Epoch [250/500], Training Loss: 0.04018944, Test Loss: 0.04400228\n",
      "Epoch [300/500], Training Loss: 0.03963970, Test Loss: 0.04511513\n",
      "Epoch [350/500], Training Loss: 0.03900105, Test Loss: 0.04449584\n",
      "Epoch [400/500], Training Loss: 0.03942160, Test Loss: 0.04542288\n",
      "Epoch [450/500], Training Loss: 0.03879860, Test Loss: 0.04399608\n",
      "Epoch [500/500], Training Loss: 0.03812746, Test Loss: 0.04389737\n",
      "**********************************************\n",
      "The reinforce process [1], collecting data ...\n",
      "Episode [0/6], Reward: 0.00017768\n",
      "Episode [1/6], Reward: 0.00019406\n",
      "Episode [2/6], Reward: 0.00025889\n",
      "Episode [3/6], Reward: 0.02440372\n",
      "Episode [4/6], Reward: 0.00028506\n",
      "Episode [5/6], Reward: 0.00027441\n",
      "Totally collect 105 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5895 training data from all previous dataset, total training sample: 6000\n",
      "Consume 196.31848788261414 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.04635050, Test Loss: 0.04831853\n",
      "Epoch [100/500], Training Loss: 0.04638940, Test Loss: 0.04861357\n",
      "Epoch [150/500], Training Loss: 0.04594463, Test Loss: 0.04940759\n",
      "Epoch [200/500], Training Loss: 0.04566165, Test Loss: 0.04800418\n",
      "Epoch [250/500], Training Loss: 0.04540235, Test Loss: 0.04845614\n",
      "Epoch [300/500], Training Loss: 0.04454664, Test Loss: 0.04745193\n",
      "Epoch [350/500], Training Loss: 0.04482200, Test Loss: 0.04712954\n",
      "Epoch [400/500], Training Loss: 0.04505566, Test Loss: 0.04657192\n",
      "Epoch [450/500], Training Loss: 0.04417427, Test Loss: 0.04632503\n",
      "Epoch [500/500], Training Loss: 0.04407539, Test Loss: 0.04719584\n",
      "**********************************************\n",
      "The reinforce process [2], collecting data ...\n",
      "Episode [0/6], Reward: 0.00007330\n",
      "Episode [1/6], Reward: 0.00017936\n",
      "Episode [2/6], Reward: 0.00018006\n",
      "Episode [3/6], Reward: 0.00018807\n",
      "Episode [4/6], Reward: 0.00019354\n",
      "Episode [5/6], Reward: 0.00019038\n",
      "Totally collect 71 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5929 training data from all previous dataset, total training sample: 6000\n",
      "Consume 144.27726793289185 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.04436038, Test Loss: 0.04600713\n",
      "Epoch [100/500], Training Loss: 0.04376618, Test Loss: 0.04689532\n",
      "Epoch [150/500], Training Loss: 0.04344349, Test Loss: 0.04752063\n",
      "Epoch [200/500], Training Loss: 0.04344621, Test Loss: 0.04545017\n",
      "Epoch [250/500], Training Loss: 0.04352716, Test Loss: 0.04604126\n",
      "Epoch [300/500], Training Loss: 0.04291563, Test Loss: 0.04742102\n",
      "Epoch [350/500], Training Loss: 0.04293586, Test Loss: 0.04521118\n",
      "Epoch [400/500], Training Loss: 0.04286281, Test Loss: 0.04612064\n",
      "Epoch [450/500], Training Loss: 0.04254650, Test Loss: 0.04668690\n",
      "Epoch [500/500], Training Loss: 0.04274432, Test Loss: 0.04690117\n",
      "**********************************************\n",
      "The reinforce process [3], collecting data ...\n",
      "Episode [0/6], Reward: 0.00018751\n",
      "Episode [1/6], Reward: 0.00020392\n",
      "Episode [2/6], Reward: 0.00028706\n",
      "Episode [3/6], Reward: 0.00029226\n",
      "Episode [4/6], Reward: 0.00007249\n",
      "Episode [5/6], Reward: 0.00031027\n",
      "Totally collect 80 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5920 training data from all previous dataset, total training sample: 6000\n",
      "Consume 159.84780859947205 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.04234122, Test Loss: 0.04484114\n",
      "Epoch [100/500], Training Loss: 0.04309385, Test Loss: 0.04268842\n",
      "Epoch [150/500], Training Loss: 0.04189417, Test Loss: 0.04291613\n",
      "Epoch [200/500], Training Loss: 0.04274259, Test Loss: 0.04359529\n",
      "Epoch [250/500], Training Loss: 0.04154420, Test Loss: 0.04269661\n",
      "Epoch [300/500], Training Loss: 0.04163976, Test Loss: 0.04299971\n",
      "Epoch [350/500], Training Loss: 0.04150901, Test Loss: 0.04308613\n",
      "Epoch [400/500], Training Loss: 0.04145690, Test Loss: 0.04445112\n",
      "Epoch [450/500], Training Loss: 0.04110787, Test Loss: 0.04405761\n",
      "Epoch [500/500], Training Loss: 0.04138225, Test Loss: 0.04345503\n",
      "**********************************************\n",
      "The reinforce process [4], collecting data ...\n",
      "Episode [0/6], Reward: 0.00081882\n",
      "Episode [1/6], Reward: 0.00018846\n",
      "Episode [2/6], Reward: 0.00018408\n",
      "Episode [3/6], Reward: 0.00020750\n",
      "Episode [4/6], Reward: 0.00026406\n",
      "Episode [5/6], Reward: 0.00018534\n",
      "Totally collect 73 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5927 training data from all previous dataset, total training sample: 6000\n",
      "Consume 138.38995718955994 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.04232764, Test Loss: 0.04291417\n",
      "Epoch [100/500], Training Loss: 0.04215084, Test Loss: 0.04235899\n",
      "Epoch [150/500], Training Loss: 0.04186085, Test Loss: 0.04279192\n",
      "Epoch [200/500], Training Loss: 0.04172616, Test Loss: 0.04271548\n",
      "Epoch [250/500], Training Loss: 0.04153864, Test Loss: 0.04375368\n",
      "Epoch [300/500], Training Loss: 0.04072148, Test Loss: 0.04240038\n",
      "Epoch [350/500], Training Loss: 0.04109470, Test Loss: 0.04366106\n",
      "Epoch [400/500], Training Loss: 0.04095126, Test Loss: 0.04379803\n",
      "Epoch [450/500], Training Loss: 0.04051384, Test Loss: 0.04627318\n",
      "Epoch [500/500], Training Loss: 0.04047056, Test Loss: 0.04394551\n",
      "**********************************************\n",
      "The reinforce process [5], collecting data ...\n",
      "Episode [0/6], Reward: 1.19002709\n",
      "Episode [1/6], Reward: 0.00115428\n",
      "Episode [2/6], Reward: 0.00033229\n",
      "Episode [3/6], Reward: 0.14639057\n",
      "Episode [4/6], Reward: 0.00018625\n",
      "Episode [5/6], Reward: 0.08484867\n",
      "Totally collect 350 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5650 training data from all previous dataset, total training sample: 6000\n",
      "Consume 674.9637954235077 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.03856262, Test Loss: 0.04117874\n",
      "Epoch [100/500], Training Loss: 0.03833467, Test Loss: 0.04179757\n",
      "Epoch [150/500], Training Loss: 0.03784734, Test Loss: 0.04008432\n",
      "Epoch [200/500], Training Loss: 0.03742309, Test Loss: 0.04093784\n",
      "Epoch [250/500], Training Loss: 0.03718531, Test Loss: 0.04056611\n",
      "Epoch [300/500], Training Loss: 0.03703264, Test Loss: 0.04276144\n",
      "Epoch [350/500], Training Loss: 0.03685285, Test Loss: 0.04139146\n",
      "Epoch [400/500], Training Loss: 0.03697958, Test Loss: 0.04102220\n",
      "Epoch [450/500], Training Loss: 0.03689416, Test Loss: 0.04137156\n",
      "Epoch [500/500], Training Loss: 0.03656241, Test Loss: 0.04201858\n",
      "**********************************************\n",
      "The reinforce process [6], collecting data ...\n",
      "Episode [0/6], Reward: 0.09051105\n",
      "Episode [1/6], Reward: 0.07600595\n",
      "Episode [2/6], Reward: 0.00008406\n",
      "Episode [3/6], Reward: 0.00008332\n",
      "Episode [4/6], Reward: 2.29386106\n",
      "Episode [5/6], Reward: 1.68270586\n",
      "Totally collect 632 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5368 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1195.5212829113007 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.03846580, Test Loss: 0.04133184\n",
      "Epoch [100/500], Training Loss: 0.03762291, Test Loss: 0.04127671\n",
      "Epoch [150/500], Training Loss: 0.03782836, Test Loss: 0.04221261\n",
      "Epoch [200/500], Training Loss: 0.03742663, Test Loss: 0.04120842\n",
      "Epoch [250/500], Training Loss: 0.03676284, Test Loss: 0.04116188\n",
      "Epoch [300/500], Training Loss: 0.03660470, Test Loss: 0.04109844\n",
      "Epoch [350/500], Training Loss: 0.03669611, Test Loss: 0.04174526\n",
      "Epoch [400/500], Training Loss: 0.03659821, Test Loss: 0.04248194\n",
      "Epoch [450/500], Training Loss: 0.03640279, Test Loss: 0.04188407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/500], Training Loss: 0.03648630, Test Loss: 0.04470662\n",
      "**********************************************\n",
      "The reinforce process [7], collecting data ...\n",
      "Episode [0/6], Reward: 0.57483667\n",
      "Episode [1/6], Reward: 0.18714069\n",
      "Episode [2/6], Reward: 0.84992404\n",
      "Episode [3/6], Reward: 0.69224064\n",
      "Episode [4/6], Reward: 0.26742958\n",
      "Episode [5/6], Reward: 0.64263535\n",
      "Totally collect 809 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5191 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1527.7383563518524 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.03172803, Test Loss: 0.03473273\n",
      "Epoch [100/500], Training Loss: 0.03122403, Test Loss: 0.03641728\n",
      "Epoch [150/500], Training Loss: 0.03097511, Test Loss: 0.03565848\n",
      "Epoch [200/500], Training Loss: 0.03028270, Test Loss: 0.03621652\n",
      "Epoch [250/500], Training Loss: 0.03050216, Test Loss: 0.03693208\n",
      "Epoch [300/500], Training Loss: 0.03006740, Test Loss: 0.03637274\n",
      "Epoch [350/500], Training Loss: 0.03007810, Test Loss: 0.03650929\n",
      "Epoch [400/500], Training Loss: 0.02985120, Test Loss: 0.03613417\n",
      "Epoch [450/500], Training Loss: 0.02980897, Test Loss: 0.03554950\n",
      "Epoch [500/500], Training Loss: 0.02971695, Test Loss: 0.03540729\n",
      "**********************************************\n",
      "The reinforce process [8], collecting data ...\n",
      "Episode [0/6], Reward: 0.00113257\n",
      "Episode [1/6], Reward: 0.23691283\n",
      "Episode [2/6], Reward: 0.00106391\n",
      "Episode [3/6], Reward: 0.23814460\n",
      "Episode [4/6], Reward: 0.00125431\n",
      "Episode [5/6], Reward: 0.00113619\n",
      "Totally collect 172 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5828 training data from all previous dataset, total training sample: 6000\n",
      "Consume 319.9073269367218 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.03548576, Test Loss: 0.03801379\n",
      "Epoch [100/500], Training Loss: 0.03477169, Test Loss: 0.03788395\n",
      "Epoch [150/500], Training Loss: 0.03391454, Test Loss: 0.03835383\n",
      "Epoch [200/500], Training Loss: 0.03404504, Test Loss: 0.03860173\n",
      "Epoch [250/500], Training Loss: 0.03392892, Test Loss: 0.03727479\n",
      "Epoch [300/500], Training Loss: 0.03369693, Test Loss: 0.03782270\n",
      "Epoch [350/500], Training Loss: 0.03363414, Test Loss: 0.03922759\n",
      "Epoch [400/500], Training Loss: 0.03309938, Test Loss: 0.03772637\n",
      "Epoch [450/500], Training Loss: 0.03332442, Test Loss: 0.03793574\n",
      "Epoch [500/500], Training Loss: 0.03310833, Test Loss: 0.03731536\n",
      "**********************************************\n",
      "The reinforce process [9], collecting data ...\n",
      "Episode [0/6], Reward: 1.65174980\n",
      "Episode [1/6], Reward: 2.36683522\n",
      "Episode [2/6], Reward: 0.00013726\n",
      "Episode [3/6], Reward: 0.80850113\n",
      "Episode [4/6], Reward: 0.53752776\n",
      "Episode [5/6], Reward: 2.55600206\n",
      "Totally collect 1055 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 4945 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1985.5868039131165 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.03035276, Test Loss: 0.03524722\n",
      "Epoch [100/500], Training Loss: 0.02981625, Test Loss: 0.03601594\n",
      "Epoch [150/500], Training Loss: 0.02974857, Test Loss: 0.03549851\n",
      "Epoch [200/500], Training Loss: 0.02911113, Test Loss: 0.03549642\n",
      "Epoch [250/500], Training Loss: 0.02870161, Test Loss: 0.03544253\n",
      "Epoch [300/500], Training Loss: 0.02866153, Test Loss: 0.03548306\n",
      "Epoch [350/500], Training Loss: 0.02834806, Test Loss: 0.03642569\n",
      "Epoch [400/500], Training Loss: 0.02858906, Test Loss: 0.03602559\n",
      "Epoch [450/500], Training Loss: 0.02809035, Test Loss: 0.03605225\n",
      "Epoch [500/500], Training Loss: 0.02828682, Test Loss: 0.03554827\n",
      "**********************************************\n",
      "The reinforce process [10], collecting data ...\n",
      "Episode [0/6], Reward: 0.21449189\n",
      "Episode [1/6], Reward: 0.72551616\n",
      "Episode [2/6], Reward: 0.00110362\n",
      "Episode [3/6], Reward: 0.00076220\n",
      "Episode [4/6], Reward: 0.00012074\n",
      "Episode [5/6], Reward: 0.43711919\n",
      "Totally collect 340 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5660 training data from all previous dataset, total training sample: 6000\n",
      "Consume 634.7626152038574 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.03197867, Test Loss: 0.03446373\n",
      "Epoch [100/500], Training Loss: 0.03119839, Test Loss: 0.03614520\n",
      "Epoch [150/500], Training Loss: 0.03081773, Test Loss: 0.03580071\n",
      "Epoch [200/500], Training Loss: 0.03075818, Test Loss: 0.03518552\n",
      "Epoch [250/500], Training Loss: 0.03058539, Test Loss: 0.03631903\n",
      "Epoch [300/500], Training Loss: 0.03030206, Test Loss: 0.03515200\n",
      "Epoch [350/500], Training Loss: 0.03041689, Test Loss: 0.03517111\n",
      "Epoch [400/500], Training Loss: 0.03010377, Test Loss: 0.03480079\n",
      "Epoch [450/500], Training Loss: 0.03019802, Test Loss: 0.03457216\n",
      "Epoch [500/500], Training Loss: 0.03008028, Test Loss: 0.03512633\n",
      "**********************************************\n",
      "The reinforce process [11], collecting data ...\n",
      "Episode [0/6], Reward: 0.27109273\n",
      "Episode [1/6], Reward: 0.39501674\n",
      "Episode [2/6], Reward: 1.59103659\n",
      "Episode [3/6], Reward: 0.74379516\n",
      "Episode [4/6], Reward: 0.51784746\n",
      "Episode [5/6], Reward: 0.00052150\n",
      "Totally collect 624 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5376 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1183.93794298172 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.03072134, Test Loss: 0.03434161\n",
      "Epoch [100/500], Training Loss: 0.03040721, Test Loss: 0.03596450\n",
      "Epoch [150/500], Training Loss: 0.03032405, Test Loss: 0.03465470\n",
      "Epoch [200/500], Training Loss: 0.03023781, Test Loss: 0.03383320\n",
      "Epoch [250/500], Training Loss: 0.03009371, Test Loss: 0.03421271\n",
      "Epoch [300/500], Training Loss: 0.02976740, Test Loss: 0.03412472\n",
      "Epoch [350/500], Training Loss: 0.02933475, Test Loss: 0.03455069\n",
      "Epoch [400/500], Training Loss: 0.02961945, Test Loss: 0.03541628\n",
      "Epoch [450/500], Training Loss: 0.02922339, Test Loss: 0.03439012\n",
      "Epoch [500/500], Training Loss: 0.02953938, Test Loss: 0.03549199\n",
      "**********************************************\n",
      "The reinforce process [12], collecting data ...\n",
      "Episode [0/6], Reward: 0.20167854\n",
      "Episode [1/6], Reward: 0.00136843\n",
      "Episode [2/6], Reward: 0.00115642\n",
      "Episode [3/6], Reward: 0.00019620\n",
      "Episode [4/6], Reward: 0.00122336\n",
      "Episode [5/6], Reward: 0.00012044\n",
      "Totally collect 140 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5860 training data from all previous dataset, total training sample: 6000\n",
      "Consume 264.30182695388794 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.03192712, Test Loss: 0.03466091\n",
      "Epoch [100/500], Training Loss: 0.03136380, Test Loss: 0.03467616\n",
      "Epoch [150/500], Training Loss: 0.03088637, Test Loss: 0.03378321\n",
      "Epoch [200/500], Training Loss: 0.03050467, Test Loss: 0.03426075\n",
      "Epoch [250/500], Training Loss: 0.03051507, Test Loss: 0.03559774\n",
      "Epoch [300/500], Training Loss: 0.03075685, Test Loss: 0.03426073\n",
      "Epoch [350/500], Training Loss: 0.03041101, Test Loss: 0.03392206\n",
      "Epoch [400/500], Training Loss: 0.03017148, Test Loss: 0.03419205\n",
      "Epoch [450/500], Training Loss: 0.02993947, Test Loss: 0.03468869\n",
      "Epoch [500/500], Training Loss: 0.03010782, Test Loss: 0.03524024\n",
      "**********************************************\n",
      "The reinforce process [13], collecting data ...\n",
      "Episode [0/6], Reward: 1.49958413\n",
      "Episode [1/6], Reward: 0.00011859\n",
      "Episode [2/6], Reward: 1.75964902\n",
      "Episode [3/6], Reward: 1.84324288\n",
      "Episode [4/6], Reward: 0.33326699\n",
      "Episode [5/6], Reward: 0.00018743\n",
      "Totally collect 915 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5085 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1712.702550649643 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02855258, Test Loss: 0.03267349\n",
      "Epoch [100/500], Training Loss: 0.02744771, Test Loss: 0.03227804\n",
      "Epoch [150/500], Training Loss: 0.02726752, Test Loss: 0.03190219\n",
      "Epoch [200/500], Training Loss: 0.02677402, Test Loss: 0.03404987\n",
      "Epoch [250/500], Training Loss: 0.02663314, Test Loss: 0.03274268\n",
      "Epoch [300/500], Training Loss: 0.02688519, Test Loss: 0.03349703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [350/500], Training Loss: 0.02639268, Test Loss: 0.03284256\n",
      "Epoch [400/500], Training Loss: 0.02678757, Test Loss: 0.03338470\n",
      "Epoch [450/500], Training Loss: 0.02632196, Test Loss: 0.03362174\n",
      "Epoch [500/500], Training Loss: 0.02644313, Test Loss: 0.03370508\n",
      "**********************************************\n",
      "The reinforce process [14], collecting data ...\n",
      "Episode [0/6], Reward: 0.00116554\n",
      "Episode [1/6], Reward: 0.00120330\n",
      "Episode [2/6], Reward: 0.00107961\n",
      "Episode [3/6], Reward: 0.00119055\n",
      "Episode [4/6], Reward: 0.00116842\n",
      "Episode [5/6], Reward: 0.00085223\n",
      "Totally collect 83 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5917 training data from all previous dataset, total training sample: 6000\n",
      "Consume 155.87657189369202 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02966310, Test Loss: 0.03238061\n",
      "Epoch [100/500], Training Loss: 0.02915360, Test Loss: 0.03245166\n",
      "Epoch [150/500], Training Loss: 0.02910444, Test Loss: 0.03243264\n",
      "Epoch [200/500], Training Loss: 0.02856614, Test Loss: 0.03250644\n",
      "Epoch [250/500], Training Loss: 0.02878855, Test Loss: 0.03368816\n",
      "Epoch [300/500], Training Loss: 0.02855611, Test Loss: 0.03356894\n",
      "Epoch [350/500], Training Loss: 0.02819615, Test Loss: 0.03350323\n",
      "Epoch [400/500], Training Loss: 0.02839356, Test Loss: 0.03276729\n",
      "Epoch [450/500], Training Loss: 0.02832004, Test Loss: 0.03384953\n",
      "Epoch [500/500], Training Loss: 0.02838004, Test Loss: 0.03364111\n",
      "**********************************************\n",
      "The reinforce process [15], collecting data ...\n",
      "Episode [0/6], Reward: 0.00076393\n",
      "Episode [1/6], Reward: 0.00038160\n",
      "Episode [2/6], Reward: 0.00037417\n",
      "Episode [3/6], Reward: 0.00036444\n",
      "Episode [4/6], Reward: 0.00035279\n",
      "Episode [5/6], Reward: 0.00038978\n",
      "Totally collect 73 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5927 training data from all previous dataset, total training sample: 6000\n",
      "Consume 139.0772545337677 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02899555, Test Loss: 0.03250031\n",
      "Epoch [100/500], Training Loss: 0.02870512, Test Loss: 0.03272946\n",
      "Epoch [150/500], Training Loss: 0.02827253, Test Loss: 0.03258737\n",
      "Epoch [200/500], Training Loss: 0.02798882, Test Loss: 0.03278504\n",
      "Epoch [250/500], Training Loss: 0.02804754, Test Loss: 0.03331782\n",
      "Epoch [300/500], Training Loss: 0.02814369, Test Loss: 0.03387199\n",
      "Epoch [350/500], Training Loss: 0.02771779, Test Loss: 0.03352446\n",
      "Epoch [400/500], Training Loss: 0.02749809, Test Loss: 0.03373159\n",
      "Epoch [450/500], Training Loss: 0.02728946, Test Loss: 0.03285351\n",
      "Epoch [500/500], Training Loss: 0.02725059, Test Loss: 0.03321325\n",
      "**********************************************\n",
      "The reinforce process [16], collecting data ...\n",
      "Episode [0/6], Reward: 0.00124845\n",
      "Episode [1/6], Reward: 2.31368445\n",
      "Episode [2/6], Reward: 0.32336807\n",
      "Episode [3/6], Reward: 0.00117227\n",
      "Episode [4/6], Reward: 1.36535282\n",
      "Episode [5/6], Reward: 0.00115999\n",
      "Totally collect 707 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5293 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1321.8648023605347 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02632554, Test Loss: 0.03050941\n",
      "Epoch [100/500], Training Loss: 0.02554357, Test Loss: 0.03120567\n",
      "Epoch [150/500], Training Loss: 0.02536867, Test Loss: 0.03115809\n",
      "Epoch [200/500], Training Loss: 0.02584829, Test Loss: 0.03113281\n",
      "Epoch [250/500], Training Loss: 0.02517214, Test Loss: 0.03139021\n",
      "Epoch [300/500], Training Loss: 0.02531693, Test Loss: 0.03093565\n",
      "Epoch [350/500], Training Loss: 0.02513068, Test Loss: 0.03079853\n",
      "Epoch [400/500], Training Loss: 0.02490869, Test Loss: 0.03219303\n",
      "Epoch [450/500], Training Loss: 0.02520163, Test Loss: 0.03247545\n",
      "Epoch [500/500], Training Loss: 0.02513797, Test Loss: 0.03182037\n",
      "**********************************************\n",
      "The reinforce process [17], collecting data ...\n",
      "Episode [0/6], Reward: 0.00108961\n",
      "Episode [1/6], Reward: 0.00116101\n",
      "Episode [2/6], Reward: 0.00116751\n",
      "Episode [3/6], Reward: 1.91638713\n",
      "Episode [4/6], Reward: 0.00109689\n",
      "Episode [5/6], Reward: 0.00117745\n",
      "Totally collect 268 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5732 training data from all previous dataset, total training sample: 6000\n",
      "Consume 516.8007125854492 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02695000, Test Loss: 0.03157659\n",
      "Epoch [100/500], Training Loss: 0.02609877, Test Loss: 0.03030806\n",
      "Epoch [150/500], Training Loss: 0.02582031, Test Loss: 0.03104041\n",
      "Epoch [200/500], Training Loss: 0.02594350, Test Loss: 0.03050282\n",
      "Epoch [250/500], Training Loss: 0.02602701, Test Loss: 0.03151579\n",
      "Epoch [300/500], Training Loss: 0.02604218, Test Loss: 0.03143696\n",
      "Epoch [350/500], Training Loss: 0.02558114, Test Loss: 0.03073268\n",
      "Epoch [400/500], Training Loss: 0.02547462, Test Loss: 0.03155670\n",
      "Epoch [450/500], Training Loss: 0.02539600, Test Loss: 0.03155129\n",
      "Epoch [500/500], Training Loss: 0.02545476, Test Loss: 0.03260793\n",
      "**********************************************\n",
      "The reinforce process [18], collecting data ...\n",
      "Episode [0/6], Reward: 0.00125041\n",
      "Episode [1/6], Reward: 0.42965827\n",
      "Episode [2/6], Reward: 0.00111615\n",
      "Episode [3/6], Reward: 0.25648624\n",
      "Episode [4/6], Reward: 0.00108284\n",
      "Episode [5/6], Reward: 0.63276499\n",
      "Totally collect 258 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5742 training data from all previous dataset, total training sample: 6000\n",
      "Consume 477.20138454437256 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02721586, Test Loss: 0.03195346\n",
      "Epoch [100/500], Training Loss: 0.02676144, Test Loss: 0.03044874\n",
      "Epoch [150/500], Training Loss: 0.02664770, Test Loss: 0.03106499\n",
      "Epoch [200/500], Training Loss: 0.02602551, Test Loss: 0.03123350\n",
      "Epoch [250/500], Training Loss: 0.02596141, Test Loss: 0.03139075\n",
      "Epoch [300/500], Training Loss: 0.02599883, Test Loss: 0.03088103\n",
      "Epoch [350/500], Training Loss: 0.02563515, Test Loss: 0.03091701\n",
      "Epoch [400/500], Training Loss: 0.02569969, Test Loss: 0.03172101\n",
      "Epoch [450/500], Training Loss: 0.02558446, Test Loss: 0.03136346\n",
      "Epoch [500/500], Training Loss: 0.02588004, Test Loss: 0.03122102\n",
      "**********************************************\n",
      "The reinforce process [19], collecting data ...\n",
      "Episode [0/6], Reward: 0.00033643\n",
      "Episode [1/6], Reward: 0.00122964\n",
      "Episode [2/6], Reward: 0.00033959\n",
      "Episode [3/6], Reward: 0.76732103\n",
      "Episode [4/6], Reward: 0.00033374\n",
      "Episode [5/6], Reward: 0.00079883\n",
      "Totally collect 227 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5773 training data from all previous dataset, total training sample: 6000\n",
      "Consume 434.7346668243408 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02740377, Test Loss: 0.03086221\n",
      "Epoch [100/500], Training Loss: 0.02643330, Test Loss: 0.02993041\n",
      "Epoch [150/500], Training Loss: 0.02645030, Test Loss: 0.03119338\n",
      "Epoch [200/500], Training Loss: 0.02616328, Test Loss: 0.03204880\n",
      "Epoch [250/500], Training Loss: 0.02620762, Test Loss: 0.03158637\n",
      "Epoch [300/500], Training Loss: 0.02576639, Test Loss: 0.03052077\n",
      "Epoch [350/500], Training Loss: 0.02580047, Test Loss: 0.03051115\n",
      "Epoch [400/500], Training Loss: 0.02559490, Test Loss: 0.03197954\n",
      "Epoch [450/500], Training Loss: 0.02562973, Test Loss: 0.03255576\n",
      "Epoch [500/500], Training Loss: 0.02578242, Test Loss: 0.03050131\n",
      "**********************************************\n",
      "The reinforce process [20], collecting data ...\n",
      "Episode [0/6], Reward: 0.00082891\n",
      "Episode [1/6], Reward: 0.00030725\n",
      "Episode [2/6], Reward: 0.00167335\n",
      "Episode [3/6], Reward: 0.00088841\n",
      "Episode [4/6], Reward: 0.00090485\n",
      "Episode [5/6], Reward: 0.00027305\n",
      "Totally collect 83 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5917 training data from all previous dataset, total training sample: 6000\n",
      "Consume 161.88661885261536 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02836370, Test Loss: 0.03026126\n",
      "Epoch [100/500], Training Loss: 0.02808813, Test Loss: 0.03019115\n",
      "Epoch [150/500], Training Loss: 0.02774908, Test Loss: 0.03029058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/500], Training Loss: 0.02774063, Test Loss: 0.03001721\n",
      "Epoch [250/500], Training Loss: 0.02747039, Test Loss: 0.03078489\n",
      "Epoch [300/500], Training Loss: 0.02740063, Test Loss: 0.03091745\n",
      "Epoch [350/500], Training Loss: 0.02730600, Test Loss: 0.03065391\n",
      "Epoch [400/500], Training Loss: 0.02729819, Test Loss: 0.03065831\n",
      "Epoch [450/500], Training Loss: 0.02714873, Test Loss: 0.03027109\n",
      "Epoch [500/500], Training Loss: 0.02678542, Test Loss: 0.03076921\n",
      "**********************************************\n",
      "The reinforce process [21], collecting data ...\n",
      "Episode [0/6], Reward: 0.00040992\n",
      "Episode [1/6], Reward: 0.00076787\n",
      "Episode [2/6], Reward: 0.00077613\n",
      "Episode [3/6], Reward: 0.00014220\n",
      "Episode [4/6], Reward: 0.19627393\n",
      "Episode [5/6], Reward: 0.00077619\n",
      "Totally collect 125 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5875 training data from all previous dataset, total training sample: 6000\n",
      "Consume 232.03112173080444 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02707954, Test Loss: 0.03002951\n",
      "Epoch [100/500], Training Loss: 0.02630355, Test Loss: 0.02860095\n",
      "Epoch [150/500], Training Loss: 0.02590313, Test Loss: 0.02916008\n",
      "Epoch [200/500], Training Loss: 0.02553536, Test Loss: 0.02909251\n",
      "Epoch [250/500], Training Loss: 0.02563504, Test Loss: 0.02913238\n",
      "Epoch [300/500], Training Loss: 0.02535360, Test Loss: 0.02878857\n",
      "Epoch [350/500], Training Loss: 0.02515836, Test Loss: 0.02929928\n",
      "Epoch [400/500], Training Loss: 0.02524713, Test Loss: 0.02995091\n",
      "Epoch [450/500], Training Loss: 0.02529413, Test Loss: 0.03039875\n",
      "Epoch [500/500], Training Loss: 0.02512493, Test Loss: 0.02984522\n",
      "**********************************************\n",
      "The reinforce process [22], collecting data ...\n",
      "Episode [0/6], Reward: 0.00036352\n",
      "Episode [1/6], Reward: 0.00032466\n",
      "Episode [2/6], Reward: 0.00034862\n",
      "Episode [3/6], Reward: 0.00034633\n",
      "Episode [4/6], Reward: 0.00033139\n",
      "Episode [5/6], Reward: 0.00033242\n",
      "Totally collect 72 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5928 training data from all previous dataset, total training sample: 6000\n",
      "Consume 132.86674737930298 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02728637, Test Loss: 0.03010980\n",
      "Epoch [100/500], Training Loss: 0.02676060, Test Loss: 0.03033075\n",
      "Epoch [150/500], Training Loss: 0.02665089, Test Loss: 0.03029109\n",
      "Epoch [200/500], Training Loss: 0.02630898, Test Loss: 0.03013677\n",
      "Epoch [250/500], Training Loss: 0.02656017, Test Loss: 0.03049459\n",
      "Epoch [300/500], Training Loss: 0.02611685, Test Loss: 0.03059869\n",
      "Epoch [350/500], Training Loss: 0.02640124, Test Loss: 0.03095363\n",
      "Epoch [400/500], Training Loss: 0.02574285, Test Loss: 0.03132296\n",
      "Epoch [450/500], Training Loss: 0.02562490, Test Loss: 0.03066655\n",
      "Epoch [500/500], Training Loss: 0.02558234, Test Loss: 0.03075815\n",
      "**********************************************\n",
      "The reinforce process [23], collecting data ...\n",
      "Episode [0/6], Reward: 0.00119032\n",
      "Episode [1/6], Reward: 1.74503064\n",
      "Episode [2/6], Reward: 0.00078767\n",
      "Episode [3/6], Reward: 0.00084677\n",
      "Episode [4/6], Reward: 0.00030163\n",
      "Episode [5/6], Reward: 0.00110837\n",
      "Totally collect 269 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5731 training data from all previous dataset, total training sample: 6000\n",
      "Consume 496.44707202911377 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02577437, Test Loss: 0.02886833\n",
      "Epoch [100/500], Training Loss: 0.02524013, Test Loss: 0.02823849\n",
      "Epoch [150/500], Training Loss: 0.02495837, Test Loss: 0.02940986\n",
      "Epoch [200/500], Training Loss: 0.02476833, Test Loss: 0.02906523\n",
      "Epoch [250/500], Training Loss: 0.02470639, Test Loss: 0.02897225\n",
      "Epoch [300/500], Training Loss: 0.02473155, Test Loss: 0.02942565\n",
      "Epoch [350/500], Training Loss: 0.02481022, Test Loss: 0.02957639\n",
      "Epoch [400/500], Training Loss: 0.02447641, Test Loss: 0.02892277\n",
      "Epoch [450/500], Training Loss: 0.02422627, Test Loss: 0.03005718\n",
      "Epoch [500/500], Training Loss: 0.02420541, Test Loss: 0.02956474\n",
      "**********************************************\n",
      "The reinforce process [24], collecting data ...\n",
      "Episode [0/6], Reward: 1.95148229\n",
      "Episode [1/6], Reward: 0.00040270\n",
      "Episode [2/6], Reward: 1.07282252\n",
      "Episode [3/6], Reward: 1.45600794\n",
      "Episode [4/6], Reward: 0.00070562\n",
      "Episode [5/6], Reward: 0.00079883\n",
      "Totally collect 552 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5448 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1052.1629252433777 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02673188, Test Loss: 0.02984934\n",
      "Epoch [100/500], Training Loss: 0.02667141, Test Loss: 0.02994258\n",
      "Epoch [150/500], Training Loss: 0.02614040, Test Loss: 0.02959825\n",
      "Epoch [200/500], Training Loss: 0.02589929, Test Loss: 0.02951612\n",
      "Epoch [250/500], Training Loss: 0.02592547, Test Loss: 0.03020501\n",
      "Epoch [300/500], Training Loss: 0.02536107, Test Loss: 0.02999546\n",
      "Epoch [350/500], Training Loss: 0.02556527, Test Loss: 0.03022988\n",
      "Epoch [400/500], Training Loss: 0.02529383, Test Loss: 0.03004681\n",
      "Epoch [450/500], Training Loss: 0.02509028, Test Loss: 0.03014788\n",
      "Epoch [500/500], Training Loss: 0.02498683, Test Loss: 0.03086454\n",
      "**********************************************\n",
      "The reinforce process [25], collecting data ...\n",
      "Episode [0/6], Reward: 0.00078333\n",
      "Episode [1/6], Reward: 0.00101770\n",
      "Episode [2/6], Reward: 0.89623084\n",
      "Episode [3/6], Reward: 0.00116146\n",
      "Episode [4/6], Reward: 0.00081641\n",
      "Episode [5/6], Reward: 0.00114919\n",
      "Totally collect 244 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5756 training data from all previous dataset, total training sample: 6000\n",
      "Consume 465.5561788082123 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02476321, Test Loss: 0.03028258\n",
      "Epoch [100/500], Training Loss: 0.02441246, Test Loss: 0.02888825\n",
      "Epoch [150/500], Training Loss: 0.02434034, Test Loss: 0.02918192\n",
      "Epoch [200/500], Training Loss: 0.02412838, Test Loss: 0.02912123\n",
      "Epoch [250/500], Training Loss: 0.02394038, Test Loss: 0.03069817\n",
      "Epoch [300/500], Training Loss: 0.02382160, Test Loss: 0.03008355\n",
      "Epoch [350/500], Training Loss: 0.02356607, Test Loss: 0.02982022\n",
      "Epoch [400/500], Training Loss: 0.02373942, Test Loss: 0.03048097\n",
      "Epoch [450/500], Training Loss: 0.02322873, Test Loss: 0.03057311\n",
      "Epoch [500/500], Training Loss: 0.02350036, Test Loss: 0.03043777\n",
      "**********************************************\n",
      "The reinforce process [26], collecting data ...\n",
      "Episode [0/6], Reward: 0.00061424\n",
      "Episode [1/6], Reward: 0.00110236\n",
      "Episode [2/6], Reward: 0.00079823\n",
      "Episode [3/6], Reward: 0.00082552\n",
      "Episode [4/6], Reward: 0.00060835\n",
      "Episode [5/6], Reward: 0.00114410\n",
      "Totally collect 85 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5915 training data from all previous dataset, total training sample: 6000\n",
      "Consume 170.2057785987854 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02607866, Test Loss: 0.02865198\n",
      "Epoch [100/500], Training Loss: 0.02569553, Test Loss: 0.02946668\n",
      "Epoch [150/500], Training Loss: 0.02520385, Test Loss: 0.02923121\n",
      "Epoch [200/500], Training Loss: 0.02530185, Test Loss: 0.02995469\n",
      "Epoch [250/500], Training Loss: 0.02509486, Test Loss: 0.02940431\n",
      "Epoch [300/500], Training Loss: 0.02514332, Test Loss: 0.02998457\n",
      "Epoch [350/500], Training Loss: 0.02500104, Test Loss: 0.03036841\n",
      "Epoch [400/500], Training Loss: 0.02438455, Test Loss: 0.03010493\n",
      "Epoch [450/500], Training Loss: 0.02466455, Test Loss: 0.03001774\n",
      "Epoch [500/500], Training Loss: 0.02455451, Test Loss: 0.02955025\n",
      "**********************************************\n",
      "The reinforce process [27], collecting data ...\n",
      "Episode [0/6], Reward: 0.69581047\n",
      "Episode [1/6], Reward: 0.00034821\n",
      "Episode [2/6], Reward: 0.00037659\n",
      "Episode [3/6], Reward: 0.00037706\n",
      "Episode [4/6], Reward: 0.00036929\n",
      "Episode [5/6], Reward: 0.00035117\n",
      "Totally collect 181 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5819 training data from all previous dataset, total training sample: 6000\n",
      "Consume 333.14220213890076 s in this iteration\n",
      "Total training step per epoch [94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500], Training Loss: 0.02567001, Test Loss: 0.02841086\n",
      "Epoch [100/500], Training Loss: 0.02533568, Test Loss: 0.02787836\n",
      "Epoch [150/500], Training Loss: 0.02504446, Test Loss: 0.02810383\n",
      "Epoch [200/500], Training Loss: 0.02458605, Test Loss: 0.02929674\n",
      "Epoch [250/500], Training Loss: 0.02477511, Test Loss: 0.02923762\n",
      "Epoch [300/500], Training Loss: 0.02435437, Test Loss: 0.02860176\n",
      "Epoch [350/500], Training Loss: 0.02450995, Test Loss: 0.02939323\n",
      "Epoch [400/500], Training Loss: 0.02445085, Test Loss: 0.02882351\n",
      "Epoch [450/500], Training Loss: 0.02450079, Test Loss: 0.02858301\n",
      "Epoch [500/500], Training Loss: 0.02410324, Test Loss: 0.02958776\n",
      "**********************************************\n",
      "The reinforce process [28], collecting data ...\n",
      "Episode [0/6], Reward: 0.00086062\n",
      "Episode [1/6], Reward: 0.00084369\n",
      "Episode [2/6], Reward: 1.28736155\n",
      "Episode [3/6], Reward: 0.57067945\n",
      "Episode [4/6], Reward: 1.03882911\n",
      "Episode [5/6], Reward: 0.00031613\n",
      "Totally collect 516 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5484 training data from all previous dataset, total training sample: 6000\n",
      "Consume 958.0635261535645 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02405211, Test Loss: 0.02774179\n",
      "Epoch [100/500], Training Loss: 0.02359955, Test Loss: 0.02776670\n",
      "Epoch [150/500], Training Loss: 0.02296630, Test Loss: 0.02812058\n",
      "Epoch [200/500], Training Loss: 0.02317477, Test Loss: 0.02849728\n",
      "Epoch [250/500], Training Loss: 0.02272191, Test Loss: 0.02828891\n",
      "Epoch [300/500], Training Loss: 0.02247691, Test Loss: 0.02872685\n",
      "Epoch [350/500], Training Loss: 0.02229438, Test Loss: 0.02918006\n",
      "Epoch [400/500], Training Loss: 0.02235886, Test Loss: 0.02835427\n",
      "Epoch [450/500], Training Loss: 0.02239124, Test Loss: 0.02872164\n",
      "Epoch [500/500], Training Loss: 0.02232605, Test Loss: 0.02833031\n",
      "**********************************************\n",
      "The reinforce process [29], collecting data ...\n",
      "Episode [0/6], Reward: 0.00033674\n",
      "Episode [1/6], Reward: 2.12104314\n",
      "Episode [2/6], Reward: 2.05605325\n",
      "Episode [3/6], Reward: 2.61160057\n",
      "Episode [4/6], Reward: 0.00014770\n",
      "Episode [5/6], Reward: 0.64183616\n",
      "Totally collect 1044 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 4956 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1974.5357387065887 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02381636, Test Loss: 0.02819069\n",
      "Epoch [100/500], Training Loss: 0.02304580, Test Loss: 0.02761758\n",
      "Epoch [150/500], Training Loss: 0.02264066, Test Loss: 0.02831528\n",
      "Epoch [200/500], Training Loss: 0.02258978, Test Loss: 0.02935024\n",
      "Epoch [250/500], Training Loss: 0.02253173, Test Loss: 0.02843061\n",
      "Epoch [300/500], Training Loss: 0.02234474, Test Loss: 0.02847700\n",
      "Epoch [350/500], Training Loss: 0.02235297, Test Loss: 0.02915094\n",
      "Epoch [400/500], Training Loss: 0.02233480, Test Loss: 0.02950203\n",
      "Epoch [450/500], Training Loss: 0.02206736, Test Loss: 0.02943155\n",
      "Epoch [500/500], Training Loss: 0.02187402, Test Loss: 0.02921232\n",
      "**********************************************\n",
      "The reinforce process [30], collecting data ...\n",
      "Episode [0/6], Reward: 0.00023019\n",
      "Episode [1/6], Reward: 0.00025362\n",
      "Episode [2/6], Reward: 0.00030126\n",
      "Episode [3/6], Reward: 0.00024039\n",
      "Episode [4/6], Reward: 0.00023392\n",
      "Episode [5/6], Reward: 0.00024902\n",
      "Totally collect 72 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5928 training data from all previous dataset, total training sample: 6000\n",
      "Consume 138.1962537765503 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02639496, Test Loss: 0.02817736\n",
      "Epoch [100/500], Training Loss: 0.02569330, Test Loss: 0.02876842\n",
      "Epoch [150/500], Training Loss: 0.02528077, Test Loss: 0.02827988\n",
      "Epoch [200/500], Training Loss: 0.02505140, Test Loss: 0.02900145\n",
      "Epoch [250/500], Training Loss: 0.02453206, Test Loss: 0.02819427\n",
      "Epoch [300/500], Training Loss: 0.02463424, Test Loss: 0.02922408\n",
      "Epoch [350/500], Training Loss: 0.02456584, Test Loss: 0.02958656\n",
      "Epoch [400/500], Training Loss: 0.02433801, Test Loss: 0.02914774\n",
      "Epoch [450/500], Training Loss: 0.02454162, Test Loss: 0.02926833\n",
      "Epoch [500/500], Training Loss: 0.02424496, Test Loss: 0.02983725\n",
      "**********************************************\n",
      "The reinforce process [31], collecting data ...\n",
      "Episode [0/6], Reward: 0.00039088\n",
      "Episode [1/6], Reward: 0.00037503\n",
      "Episode [2/6], Reward: 0.00038842\n",
      "Episode [3/6], Reward: 0.00032966\n",
      "Episode [4/6], Reward: 0.00040316\n",
      "Episode [5/6], Reward: 0.00037837\n",
      "Totally collect 72 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5928 training data from all previous dataset, total training sample: 6000\n",
      "Consume 131.63305711746216 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02438014, Test Loss: 0.02934515\n",
      "Epoch [100/500], Training Loss: 0.02386739, Test Loss: 0.02747184\n",
      "Epoch [150/500], Training Loss: 0.02341747, Test Loss: 0.02778253\n",
      "Epoch [200/500], Training Loss: 0.02343799, Test Loss: 0.02806234\n",
      "Epoch [250/500], Training Loss: 0.02320161, Test Loss: 0.02798413\n",
      "Epoch [300/500], Training Loss: 0.02291938, Test Loss: 0.02802439\n",
      "Epoch [350/500], Training Loss: 0.02319089, Test Loss: 0.02864640\n",
      "Epoch [400/500], Training Loss: 0.02289770, Test Loss: 0.02803780\n",
      "Epoch [450/500], Training Loss: 0.02268882, Test Loss: 0.02766285\n",
      "Epoch [500/500], Training Loss: 0.02274738, Test Loss: 0.02808326\n",
      "**********************************************\n",
      "The reinforce process [32], collecting data ...\n",
      "Episode [0/6], Reward: 0.00083344\n",
      "Episode [1/6], Reward: 0.00078102\n",
      "Episode [2/6], Reward: 0.00032736\n",
      "Episode [3/6], Reward: 0.32497864\n",
      "Episode [4/6], Reward: 0.00093320\n",
      "Episode [5/6], Reward: 0.00030276\n",
      "Totally collect 131 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5869 training data from all previous dataset, total training sample: 6000\n",
      "Consume 244.59186172485352 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02404320, Test Loss: 0.02759101\n",
      "Epoch [100/500], Training Loss: 0.02363145, Test Loss: 0.02749433\n",
      "Epoch [150/500], Training Loss: 0.02343588, Test Loss: 0.02828785\n",
      "Epoch [200/500], Training Loss: 0.02318648, Test Loss: 0.02908319\n",
      "Epoch [250/500], Training Loss: 0.02269163, Test Loss: 0.02772597\n",
      "Epoch [300/500], Training Loss: 0.02269616, Test Loss: 0.02772211\n",
      "Epoch [350/500], Training Loss: 0.02286373, Test Loss: 0.02874689\n",
      "Epoch [400/500], Training Loss: 0.02242837, Test Loss: 0.02820117\n",
      "Epoch [450/500], Training Loss: 0.02227151, Test Loss: 0.02792137\n",
      "Epoch [500/500], Training Loss: 0.02244396, Test Loss: 0.02841247\n",
      "**********************************************\n",
      "The reinforce process [33], collecting data ...\n",
      "Episode [0/6], Reward: 0.61395641\n",
      "Episode [1/6], Reward: 0.00077742\n",
      "Episode [2/6], Reward: 1.62896849\n",
      "Episode [3/6], Reward: 0.00088122\n",
      "Episode [4/6], Reward: 0.00042540\n",
      "Episode [5/6], Reward: 0.00078996\n",
      "Totally collect 339 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5661 training data from all previous dataset, total training sample: 6000\n",
      "Consume 660.9356045722961 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02410301, Test Loss: 0.02794496\n",
      "Epoch [100/500], Training Loss: 0.02348577, Test Loss: 0.02755202\n",
      "Epoch [150/500], Training Loss: 0.02348576, Test Loss: 0.02822641\n",
      "Epoch [200/500], Training Loss: 0.02332538, Test Loss: 0.02826027\n",
      "Epoch [250/500], Training Loss: 0.02312989, Test Loss: 0.02892988\n",
      "Epoch [300/500], Training Loss: 0.02281878, Test Loss: 0.02885101\n",
      "Epoch [350/500], Training Loss: 0.02294463, Test Loss: 0.02908947\n",
      "Epoch [400/500], Training Loss: 0.02278050, Test Loss: 0.02883102\n",
      "Epoch [450/500], Training Loss: 0.02275008, Test Loss: 0.02848550\n",
      "Epoch [500/500], Training Loss: 0.02273180, Test Loss: 0.02996841\n",
      "**********************************************\n",
      "The reinforce process [34], collecting data ...\n",
      "Episode [0/6], Reward: 1.22547816\n",
      "Episode [1/6], Reward: 1.37734814\n",
      "Episode [2/6], Reward: 1.65193012\n",
      "Episode [3/6], Reward: 1.67712496\n",
      "Episode [4/6], Reward: 0.00027641\n",
      "Episode [5/6], Reward: 0.53184087\n",
      "Totally collect 1210 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 4790 training data from all previous dataset, total training sample: 6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consume 2249.604413509369 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.01938958, Test Loss: 0.02507229\n",
      "Epoch [100/500], Training Loss: 0.01852318, Test Loss: 0.02547797\n",
      "Epoch [150/500], Training Loss: 0.01852792, Test Loss: 0.02508089\n",
      "Epoch [200/500], Training Loss: 0.01815945, Test Loss: 0.02551964\n",
      "Epoch [250/500], Training Loss: 0.01800762, Test Loss: 0.02524056\n",
      "Epoch [300/500], Training Loss: 0.01824544, Test Loss: 0.02528247\n",
      "Epoch [350/500], Training Loss: 0.01788761, Test Loss: 0.02610449\n",
      "Epoch [400/500], Training Loss: 0.01781694, Test Loss: 0.02539862\n",
      "Epoch [450/500], Training Loss: 0.01765929, Test Loss: 0.02614354\n",
      "Epoch [500/500], Training Loss: 0.01782329, Test Loss: 0.02577414\n",
      "**********************************************\n",
      "The reinforce process [35], collecting data ...\n",
      "Episode [0/6], Reward: 0.56389361\n",
      "Episode [1/6], Reward: 0.00030989\n",
      "Episode [2/6], Reward: 0.26343052\n",
      "Episode [3/6], Reward: 0.00091745\n",
      "Episode [4/6], Reward: 0.13747014\n",
      "Episode [5/6], Reward: 0.69398463\n",
      "Totally collect 383 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5617 training data from all previous dataset, total training sample: 6000\n",
      "Consume 731.309089422226 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02264247, Test Loss: 0.02597799\n",
      "Epoch [100/500], Training Loss: 0.02199367, Test Loss: 0.02608292\n",
      "Epoch [150/500], Training Loss: 0.02159293, Test Loss: 0.02666693\n",
      "Epoch [200/500], Training Loss: 0.02107020, Test Loss: 0.02674192\n",
      "Epoch [250/500], Training Loss: 0.02127733, Test Loss: 0.02630006\n",
      "Epoch [300/500], Training Loss: 0.02084421, Test Loss: 0.02607011\n",
      "Epoch [350/500], Training Loss: 0.02076915, Test Loss: 0.02681377\n",
      "Epoch [400/500], Training Loss: 0.02076785, Test Loss: 0.02713881\n",
      "Epoch [450/500], Training Loss: 0.02070343, Test Loss: 0.02672067\n",
      "Epoch [500/500], Training Loss: 0.02067050, Test Loss: 0.02646495\n",
      "**********************************************\n",
      "The reinforce process [36], collecting data ...\n",
      "Episode [0/6], Reward: 1.74671209\n",
      "Episode [1/6], Reward: 1.94555575\n",
      "Episode [2/6], Reward: 0.00109989\n",
      "Episode [3/6], Reward: 0.00110466\n",
      "Episode [4/6], Reward: 2.88687462\n",
      "Episode [5/6], Reward: 0.00110860\n",
      "Totally collect 942 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5058 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1755.4546437263489 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02344695, Test Loss: 0.02631981\n",
      "Epoch [100/500], Training Loss: 0.02289151, Test Loss: 0.02634441\n",
      "Epoch [150/500], Training Loss: 0.02257871, Test Loss: 0.02679578\n",
      "Epoch [200/500], Training Loss: 0.02203379, Test Loss: 0.02628015\n",
      "Epoch [250/500], Training Loss: 0.02196553, Test Loss: 0.02620265\n",
      "Epoch [300/500], Training Loss: 0.02190245, Test Loss: 0.02634796\n",
      "Epoch [350/500], Training Loss: 0.02182500, Test Loss: 0.02722028\n",
      "Epoch [400/500], Training Loss: 0.02184370, Test Loss: 0.02630075\n",
      "Epoch [450/500], Training Loss: 0.02149204, Test Loss: 0.02697166\n",
      "Epoch [500/500], Training Loss: 0.02206255, Test Loss: 0.02788223\n",
      "**********************************************\n",
      "The reinforce process [37], collecting data ...\n",
      "Episode [0/6], Reward: 0.00037170\n",
      "Episode [1/6], Reward: 0.00037002\n",
      "Episode [2/6], Reward: 0.00039204\n",
      "Episode [3/6], Reward: 0.00038190\n",
      "Episode [4/6], Reward: 0.00034747\n",
      "Episode [5/6], Reward: 0.00064467\n",
      "Totally collect 73 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5927 training data from all previous dataset, total training sample: 6000\n",
      "Consume 136.06190299987793 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02372531, Test Loss: 0.02641077\n",
      "Epoch [100/500], Training Loss: 0.02295111, Test Loss: 0.02615029\n",
      "Epoch [150/500], Training Loss: 0.02253562, Test Loss: 0.02624301\n",
      "Epoch [200/500], Training Loss: 0.02232022, Test Loss: 0.02693812\n",
      "Epoch [250/500], Training Loss: 0.02212720, Test Loss: 0.02682334\n",
      "Epoch [300/500], Training Loss: 0.02240545, Test Loss: 0.02698361\n",
      "Epoch [350/500], Training Loss: 0.02217706, Test Loss: 0.02692551\n",
      "Epoch [400/500], Training Loss: 0.02183680, Test Loss: 0.02670541\n",
      "Epoch [450/500], Training Loss: 0.02216959, Test Loss: 0.02658795\n",
      "Epoch [500/500], Training Loss: 0.02198631, Test Loss: 0.02684243\n",
      "**********************************************\n",
      "The reinforce process [38], collecting data ...\n",
      "Episode [0/6], Reward: 0.00121460\n",
      "Episode [1/6], Reward: 0.71364783\n",
      "Episode [2/6], Reward: 0.00032539\n",
      "Episode [3/6], Reward: 0.00028804\n",
      "Episode [4/6], Reward: 0.00031071\n",
      "Episode [5/6], Reward: 0.00019763\n",
      "Totally collect 199 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5801 training data from all previous dataset, total training sample: 6000\n",
      "Consume 366.465842962265 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02259911, Test Loss: 0.02664782\n",
      "Epoch [100/500], Training Loss: 0.02186717, Test Loss: 0.02693189\n",
      "Epoch [150/500], Training Loss: 0.02185296, Test Loss: 0.02704037\n",
      "Epoch [200/500], Training Loss: 0.02175850, Test Loss: 0.02724249\n",
      "Epoch [250/500], Training Loss: 0.02166569, Test Loss: 0.02722569\n",
      "Epoch [300/500], Training Loss: 0.02127952, Test Loss: 0.02764779\n",
      "Epoch [350/500], Training Loss: 0.02147959, Test Loss: 0.02777944\n",
      "Epoch [400/500], Training Loss: 0.02107204, Test Loss: 0.02773651\n",
      "Epoch [450/500], Training Loss: 0.02112911, Test Loss: 0.02735096\n",
      "Epoch [500/500], Training Loss: 0.02111542, Test Loss: 0.02792370\n",
      "**********************************************\n",
      "The reinforce process [39], collecting data ...\n",
      "Episode [0/6], Reward: 0.00040413\n",
      "Episode [1/6], Reward: 0.00026267\n",
      "Episode [2/6], Reward: 0.00029453\n",
      "Episode [3/6], Reward: 0.00026568\n",
      "Episode [4/6], Reward: 0.00029537\n",
      "Episode [5/6], Reward: 0.00050934\n",
      "Totally collect 73 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5927 training data from all previous dataset, total training sample: 6000\n",
      "Consume 135.69313406944275 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02346467, Test Loss: 0.02649780\n",
      "Epoch [100/500], Training Loss: 0.02264815, Test Loss: 0.02671085\n",
      "Epoch [150/500], Training Loss: 0.02230007, Test Loss: 0.02661721\n",
      "Epoch [200/500], Training Loss: 0.02216466, Test Loss: 0.02690062\n",
      "Epoch [250/500], Training Loss: 0.02202250, Test Loss: 0.02686522\n",
      "Epoch [300/500], Training Loss: 0.02193497, Test Loss: 0.02772719\n",
      "Epoch [350/500], Training Loss: 0.02181502, Test Loss: 0.02742642\n",
      "Epoch [400/500], Training Loss: 0.02178613, Test Loss: 0.02741263\n",
      "Epoch [450/500], Training Loss: 0.02150868, Test Loss: 0.02710089\n",
      "Epoch [500/500], Training Loss: 0.02125667, Test Loss: 0.02695023\n",
      "**********************************************\n",
      "The reinforce process [40], collecting data ...\n",
      "Episode [0/6], Reward: 0.00040791\n",
      "Episode [1/6], Reward: 0.00039319\n",
      "Episode [2/6], Reward: 0.00039971\n",
      "Episode [3/6], Reward: 0.00072736\n",
      "Episode [4/6], Reward: 1.29341360\n",
      "Episode [5/6], Reward: 2.31449601\n",
      "Totally collect 649 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5351 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1227.5941348075867 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02117103, Test Loss: 0.02586569\n",
      "Epoch [100/500], Training Loss: 0.02042405, Test Loss: 0.02612997\n",
      "Epoch [150/500], Training Loss: 0.02051308, Test Loss: 0.02535255\n",
      "Epoch [200/500], Training Loss: 0.02016998, Test Loss: 0.02606446\n",
      "Epoch [250/500], Training Loss: 0.02004435, Test Loss: 0.02618938\n",
      "Epoch [300/500], Training Loss: 0.01958899, Test Loss: 0.02584683\n",
      "Epoch [350/500], Training Loss: 0.01977172, Test Loss: 0.02634655\n",
      "Epoch [400/500], Training Loss: 0.01961398, Test Loss: 0.02625871\n",
      "Epoch [450/500], Training Loss: 0.01968922, Test Loss: 0.02613027\n",
      "Epoch [500/500], Training Loss: 0.01946009, Test Loss: 0.02679533\n",
      "**********************************************\n",
      "The reinforce process [41], collecting data ...\n",
      "Episode [0/6], Reward: 2.41789726\n",
      "Episode [1/6], Reward: 0.93065833\n",
      "Episode [2/6], Reward: 0.00083954\n",
      "Episode [3/6], Reward: 0.00070115\n",
      "Episode [4/6], Reward: 0.65097560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [5/6], Reward: 0.00073498\n",
      "Totally collect 487 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5513 training data from all previous dataset, total training sample: 6000\n",
      "Consume 912.558183670044 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02244645, Test Loss: 0.02639405\n",
      "Epoch [100/500], Training Loss: 0.02184075, Test Loss: 0.02689320\n",
      "Epoch [150/500], Training Loss: 0.02147497, Test Loss: 0.02597898\n",
      "Epoch [200/500], Training Loss: 0.02105368, Test Loss: 0.02617126\n",
      "Epoch [250/500], Training Loss: 0.02081544, Test Loss: 0.02663940\n",
      "Epoch [300/500], Training Loss: 0.02085124, Test Loss: 0.02644346\n",
      "Epoch [350/500], Training Loss: 0.02071889, Test Loss: 0.02661313\n",
      "Epoch [400/500], Training Loss: 0.02085002, Test Loss: 0.02689238\n",
      "Epoch [450/500], Training Loss: 0.02045535, Test Loss: 0.02707966\n",
      "Epoch [500/500], Training Loss: 0.02046172, Test Loss: 0.02715217\n",
      "**********************************************\n",
      "The reinforce process [42], collecting data ...\n",
      "Episode [0/6], Reward: 0.00107994\n",
      "Episode [1/6], Reward: 1.19370397\n",
      "Episode [2/6], Reward: 0.00193294\n",
      "Episode [3/6], Reward: 0.00066338\n",
      "Episode [4/6], Reward: 0.00121074\n",
      "Episode [5/6], Reward: 0.89078661\n",
      "Totally collect 355 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5645 training data from all previous dataset, total training sample: 6000\n",
      "Consume 658.1319644451141 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02308788, Test Loss: 0.02687389\n",
      "Epoch [100/500], Training Loss: 0.02237971, Test Loss: 0.02753386\n",
      "Epoch [150/500], Training Loss: 0.02227304, Test Loss: 0.02618766\n",
      "Epoch [200/500], Training Loss: 0.02173909, Test Loss: 0.02680766\n",
      "Epoch [250/500], Training Loss: 0.02175246, Test Loss: 0.02724031\n",
      "Epoch [300/500], Training Loss: 0.02152533, Test Loss: 0.02777736\n",
      "Epoch [350/500], Training Loss: 0.02159444, Test Loss: 0.02652795\n",
      "Epoch [400/500], Training Loss: 0.02136062, Test Loss: 0.02676208\n",
      "Epoch [450/500], Training Loss: 0.02122150, Test Loss: 0.02727819\n",
      "Epoch [500/500], Training Loss: 0.02153428, Test Loss: 0.02677898\n",
      "**********************************************\n",
      "The reinforce process [43], collecting data ...\n",
      "Episode [0/6], Reward: 0.00072856\n",
      "Episode [1/6], Reward: 0.00042358\n",
      "Episode [2/6], Reward: 0.00071239\n",
      "Episode [3/6], Reward: 0.00063281\n",
      "Episode [4/6], Reward: 0.00065256\n",
      "Episode [5/6], Reward: 0.00042122\n",
      "Totally collect 80 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5920 training data from all previous dataset, total training sample: 6000\n",
      "Consume 162.0249845981598 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02164007, Test Loss: 0.02447417\n",
      "Epoch [100/500], Training Loss: 0.02132709, Test Loss: 0.02510994\n",
      "Epoch [150/500], Training Loss: 0.02081060, Test Loss: 0.02553938\n",
      "Epoch [200/500], Training Loss: 0.02072262, Test Loss: 0.02564388\n",
      "Epoch [250/500], Training Loss: 0.02064038, Test Loss: 0.02589751\n",
      "Epoch [300/500], Training Loss: 0.02050436, Test Loss: 0.02528017\n",
      "Epoch [350/500], Training Loss: 0.02010553, Test Loss: 0.02583775\n",
      "Epoch [400/500], Training Loss: 0.02008824, Test Loss: 0.02593710\n",
      "Epoch [450/500], Training Loss: 0.02030771, Test Loss: 0.02556384\n",
      "Epoch [500/500], Training Loss: 0.02014479, Test Loss: 0.02615587\n",
      "**********************************************\n",
      "The reinforce process [44], collecting data ...\n",
      "Episode [0/6], Reward: 0.44772055\n",
      "Episode [1/6], Reward: 0.00141938\n",
      "Episode [2/6], Reward: 1.67642403\n",
      "Episode [3/6], Reward: 0.00102030\n",
      "Episode [4/6], Reward: 2.42872514\n",
      "Episode [5/6], Reward: 0.63086074\n",
      "Totally collect 777 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5223 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1474.60200715065 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02367084, Test Loss: 0.02620570\n",
      "Epoch [100/500], Training Loss: 0.02325766, Test Loss: 0.02662853\n",
      "Epoch [150/500], Training Loss: 0.02283894, Test Loss: 0.02652099\n",
      "Epoch [200/500], Training Loss: 0.02276306, Test Loss: 0.02726812\n",
      "Epoch [250/500], Training Loss: 0.02294966, Test Loss: 0.02693297\n",
      "Epoch [300/500], Training Loss: 0.02247594, Test Loss: 0.02707433\n",
      "Epoch [350/500], Training Loss: 0.02219891, Test Loss: 0.02708943\n",
      "Epoch [400/500], Training Loss: 0.02218533, Test Loss: 0.02730514\n",
      "Epoch [450/500], Training Loss: 0.02245301, Test Loss: 0.02701949\n",
      "Epoch [500/500], Training Loss: 0.02237243, Test Loss: 0.02727349\n",
      "**********************************************\n",
      "The reinforce process [45], collecting data ...\n",
      "Episode [0/6], Reward: 0.86850164\n",
      "Episode [1/6], Reward: 0.62048433\n",
      "Episode [2/6], Reward: 0.85716916\n",
      "Episode [3/6], Reward: 0.72003554\n",
      "Episode [4/6], Reward: 0.20230221\n",
      "Episode [5/6], Reward: 0.69221604\n",
      "Totally collect 808 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5192 training data from all previous dataset, total training sample: 6000\n",
      "Consume 1508.1725475788116 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02221380, Test Loss: 0.02468232\n",
      "Epoch [100/500], Training Loss: 0.02168938, Test Loss: 0.02432575\n",
      "Epoch [150/500], Training Loss: 0.02179719, Test Loss: 0.02512025\n",
      "Epoch [200/500], Training Loss: 0.02134200, Test Loss: 0.02497376\n",
      "Epoch [250/500], Training Loss: 0.02115545, Test Loss: 0.02544676\n",
      "Epoch [300/500], Training Loss: 0.02101755, Test Loss: 0.02578809\n",
      "Epoch [350/500], Training Loss: 0.02074122, Test Loss: 0.02538487\n",
      "Epoch [400/500], Training Loss: 0.02092654, Test Loss: 0.02527483\n",
      "Epoch [450/500], Training Loss: 0.02083712, Test Loss: 0.02622369\n",
      "Epoch [500/500], Training Loss: 0.02065674, Test Loss: 0.02556693\n",
      "**********************************************\n",
      "The reinforce process [46], collecting data ...\n",
      "Episode [0/6], Reward: 0.99300579\n",
      "Episode [1/6], Reward: 1.65704988\n",
      "Episode [2/6], Reward: 0.00117237\n",
      "Episode [3/6], Reward: 2.28907092\n",
      "Episode [4/6], Reward: 1.96159785\n",
      "Episode [5/6], Reward: 0.00126794\n",
      "Totally collect 1070 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 4930 training data from all previous dataset, total training sample: 6000\n",
      "Consume 2069.9416241645813 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02259541, Test Loss: 0.02615554\n",
      "Epoch [100/500], Training Loss: 0.02179935, Test Loss: 0.02639005\n",
      "Epoch [150/500], Training Loss: 0.02138952, Test Loss: 0.02769291\n",
      "Epoch [200/500], Training Loss: 0.02120862, Test Loss: 0.02723430\n",
      "Epoch [250/500], Training Loss: 0.02111694, Test Loss: 0.02660202\n",
      "Epoch [300/500], Training Loss: 0.02081851, Test Loss: 0.02745739\n",
      "Epoch [350/500], Training Loss: 0.02089312, Test Loss: 0.02706331\n",
      "Epoch [400/500], Training Loss: 0.02054626, Test Loss: 0.02698970\n",
      "Epoch [450/500], Training Loss: 0.02084255, Test Loss: 0.02815145\n",
      "Epoch [500/500], Training Loss: 0.02050072, Test Loss: 0.02758065\n",
      "**********************************************\n",
      "The reinforce process [47], collecting data ...\n",
      "Episode [0/6], Reward: 0.72704554\n",
      "Episode [1/6], Reward: 0.00079151\n",
      "Episode [2/6], Reward: 0.00035464\n",
      "Episode [3/6], Reward: 0.00037776\n",
      "Episode [4/6], Reward: 0.00043525\n",
      "Episode [5/6], Reward: 0.00022172\n",
      "Totally collect 192 data based on MPC\n",
      "Saving all datas to storage/data_exp_1.pkl\n",
      "Sample 5808 training data from all previous dataset, total training sample: 6000\n",
      "Consume 270.51067304611206 s in this iteration\n",
      "Total training step per epoch [94]\n",
      "Epoch [50/500], Training Loss: 0.02171049, Test Loss: 0.02497704\n",
      "Epoch [100/500], Training Loss: 0.02096951, Test Loss: 0.02539194\n",
      "Epoch [150/500], Training Loss: 0.02051550, Test Loss: 0.02495847\n",
      "Epoch [200/500], Training Loss: 0.02047628, Test Loss: 0.02484173\n",
      "Epoch [250/500], Training Loss: 0.02027499, Test Loss: 0.02564745\n",
      "Epoch [300/500], Training Loss: 0.02010218, Test Loss: 0.02573560\n",
      "Epoch [350/500], Training Loss: 0.02001892, Test Loss: 0.02568806\n",
      "Epoch [400/500], Training Loss: 0.01998153, Test Loss: 0.02570216\n",
      "Epoch [450/500], Training Loss: 0.02012918, Test Loss: 0.02546371\n",
      "Epoch [500/500], Training Loss: 0.01976294, Test Loss: 0.02635219\n",
      "**********************************************\n",
      "The reinforce process [48], collecting data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [0/6], Reward: 0.33024092\n",
      "Episode [1/6], Reward: 1.11588724\n",
      "Episode [2/6], Reward: 0.00085280\n",
      "Episode [3/6], Reward: 0.37894509\n"
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "for itr in range(config[\"dataset_config\"][\"n_mpc_itrs\"]):\n",
    "    t = time.time()\n",
    "    print(\"**********************************************\")\n",
    "    print(\"The reinforce process [%s], collecting data ...\" % itr)\n",
    "    rewards = data_fac.collect_mpc_dataset(mpc,model)\n",
    "    trainset, testset = data_fac.make_dataset()\n",
    "    rewards_list += rewards\n",
    "    \n",
    "    plt.close(\"all\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title('Reward Trend with %s iteration' % itr)\n",
    "    plt.plot(rewards_list)\n",
    "    plt.savefig(\"storage/reward-\" + str(model.exp_number) + \".png\")\n",
    "    print(\"Consume %s s in this iteration\" % (time.time()-t))\n",
    "    loss = model.train(trainset,testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
